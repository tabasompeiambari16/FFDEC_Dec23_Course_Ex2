

1. Setting up Kafka: Apache Kafka is a distributed streaming platform that allows you to publish and subscribe to streams of records. It provides high-throughput, fault-tolerant messaging, and is widely used for building real-time data pipelines and streaming applications. You can download Kafka from the Apache Kafka website and follow the installation instructions for your operating system.

2. Creating a Kafka topic: In Kafka, a topic is a category or feed name to which records are published. Topics are divided into partitions, and each partition is ordered and immutable. You can create a topic using the Kafka command-line tools by running the `kafka-topics.sh` script or by using a programming language client, such as Kafka-Python. You'll need to specify the topic name, the number of partitions, and other configuration options.

3. Fetching data from randomuser.me: The randomuser.me API provides a free, open-source API for generating random user data. You can make HTTP requests to the API endpoints to fetch user data in JSON format. The API offers various query parameters to control the number of users, gender, nationality, etc. You can use a programming language like Python and libraries like requests or aiohttp to make the HTTP requests and retrieve the data.

4. Publishing data to Kafka: To publish data to a Kafka topic, you'll need to use a Kafka producer client. In the producer, you specify the topic to which you want to publish the data and send the data as records. The producer can be implemented using Kafka client libraries available in different programming languages. These libraries provide APIs to send data to Kafka, handle serialization/deserialization, and manage acknowledgments.

5. Setting up the data processing pipeline: Depending on your requirements, you can choose different components or frameworks within the Kafka ecosystem to build your data processing pipeline. Some popular options include Kafka Streams, Apache Spark with Kafka integration, or custom consumer applications. Kafka Streams allows you to process and transform data directly within Kafka using a stream processing DSL. Apache Spark provides a powerful framework for distributed data processing and can integrate with Kafka for stream processing. Custom consumer applications can be developed using Kafka client libraries to consume data from Kafka, perform processing, and publish to the next step.

6. Saving data to PostgreSQL: To save data from Kafka to PostgreSQL, you'll need to develop a consumer application that reads data from Kafka, processes it, and writes it to the PostgreSQL database. You can use libraries like psycopg2 or SQLAlchemy (in Python) to interact with the database. The consumer application can be run as a separate process or service, continuously consuming data from Kafka and saving it to the database.

7. Activating Postgres backup mechanism: Postgres provides various backup and recovery mechanisms to ensure data durability and availability. The Full backup + WAL archiving method is one approach to backup and recovery. It involves taking regular full backups of the database and archiving the Write-Ahead Logs (WAL) generated by Postgres for point-in-time recovery. You can configure the archiving settings in the PostgreSQL configuration file (`postgresql.conf`) and set up a backup strategy using tools like `pg_basebackup` and WAL archiving.

8. Using nocodb.com to work with data: NocoDB is a web-based database management platform that allows you to interact with your PostgreSQL database through a visual interface. It provides features like table management, data querying, and various views (grid view, kanban view, gallery view, etc.) to work with your data. You can connect NocoDB to your PostgreSQL database by providing the necessary connection details, and then you can perform operations on your data using the NocoDB interface.

9. Setting up Git repository: To track your project code and configuration files, you can create a Git repository. Git is a version control system that allows you to track changes to your codebase, collaborate with others, and manage different branches of your project. Platforms like GitHub, GitLab, and Bitbucket provide hosting for Git repositories and offer additional features like issue tracking, pull requests, and project management tools.

Remember to follow best practices for code organization, documentation, and testing throughout your project. Regularly commit your code to the Git repository and consider using branches for different features or experiments. This will help you maintain a well-structured and manageable codebase.
# Realtime Data Streaming | End-to-End Data Engineering Project

## Introduction 
This project employs a multifaceted technological stack to establish an end-to-end data processing pipeline. The workflow commences by fetching data from the `randomuser.me` API to generate synthetic user data. This raw data is subsequently channeled through **Apache Airflow** for data orchestration and storage in a **PostgreSQL** database. 

The data is then streamed through **Apache Kafka** in conjunction with **Apache Zookeeper** to facilitate real-time data movement from PostgreSQL to the processing engine. For streamlined management and monitoring of Kafka streams, **Control Center** and **Schema Registry** are employed to handle schema configurations and ensure effective oversight of the data streams.

Subsequently, **Apache Spark** is utilized to conduct data processing tasks, following which the processed data is persisted in a **Cassandra** database, providing a durable storage solution for the refined information.

The entire pipeline is encapsulated within **Docker** containers, affording a streamlined and portable deployment mechanism. 

## System Architecture
![System Architecture](https://github.com/NitinDatta8/realtime-data-streaming/blob/main/Data%20engineering%20architecture.png)

## Technologies
- **Data Source**:  `randomuser.me` API is used to generate random user data for the pipeline.
- **Apache Airflow**: Helps with orchestrating the pipeline and storing fetched data in a PostgreSQL database.
- **Apache Kafka and Zookeeper**: Used for streaming data from PostgreSQL to the processing engine.
- **Control Center and Schema Registry**: Helps in monitoring and schema management of the Kafka streams.
- **Apache Spark**: Responsible for data processing with master and worker nodes.
- **Cassandra**: Database to store the processed data.
- **Docker**: Containerize the entire pipeline.

## Things to learn 
- Establishing a data pipeline using Apache Airflow for workflow orchestration and data management.
- Implementing real-time data streaming through Apache Kafka to facilitate data transfer and processing in real-time.
- Enabling distributed synchronization using Apache Zookeeper for robust coordination and reliability in a distributed system.
- Employing data processing techniques powered by Apache Spark for efficient and scalable data transformation and analysis.
- Utilizing data storage solutions with PostgreSQL and Cassandra to securely store and manage structured and unstructured data, respectively.
- Containerizing the entire data engineering infrastructure with Docker to ensure portability and ease of deployment across various environments.

## Acknowledgements
I would like to thank [Yusuf Ganiyu](https://www.linkedin.com/in/yusuf-ganiyu-b90140107/) for this amazing project. 


Please follow the tutorial here to build this data engineering pipeline yourself.
[YouTube Video Tutorial](https://www.youtube.com/watch?v=GqAcTrqKcrY)

